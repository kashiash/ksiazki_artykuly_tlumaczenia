You same API which is taking prices from our UCC and giving it to our Azure team. Sorry, taking it from our Azure team and giving it to the service center. Can we use the same thing and have a parallel API sending it to our Polish colleagues? The reason behind is they have their own different service center. Which is. Somehow maintained and bit better than UTA context so they can show customer final final price. So. We have currently Tobias on the on the call, which is the head of API team. So Tobias, the question is can we do that? If yes, how long will it take? If no, then what can we do better? To support our Polish colleagues because currently they have the prices. But they take it from somehow with a different methodology like they just copy paste what is coming in service center and it's a long process. So it would be good for the customers in Poland which are checking the Polish service center. So that we connected digitally. And then we should have a better interface, so that's the intention behind the discussion, I think. Yeah, for sure is possible. The thing is, so it's a pretty new API. We are using a service center with the Asia database, so new technology and we have some bottlenecks here and I think one is the performance. And. I have some concerns about this ability when we have multiple calls and so we need to know the volumes we expect. To be able to. Tell that it's possible or not, so we need to do the performance tests and so on to have some measurements and maybe we need to increase performance speed on the database level. We need to increase it on API level. So we have some some amounts here where we need to get more experience. And then for sure, I think for for. Would have possible we have that channel part, the API which can be used. Where you have already some credentials and endpoints which can be used to look up this information. When you say sending Santos, it's just. Doing it get so someone is fetching the data. Like we doing it, the service center correct or is there some different requirements? No, I think I'm also not. I have never seen the service center what they are using. It's just my imagination that OK they will not be recalling the service all the time. Maybe they would need the data once in a daytime that would be enough for them. So let's see, but these guys you guys can tell us OK what's what are the requirements? How do you imagine it? Because my mine is assumption and you have the real thing. So maybe you guys can. Tell us what's the details about this requirement. Yes, it's as you said, Santos so once or twice a day and we just gonna fetch the data. What data so why are they some basic or default data criteria or you try to fetch one millions of records? Do you expect pagination export file or what? What? Yeah, what will be the outcome? The best option in my in my opinion is take everything at once. But I'm not sure if the service is. It is possible to get this data in one request. If not, then we can divide this request for each country in example or something like that. So this is not a general behavior of an API to make some mass download. So normally we support something like pagination so you have 100 records per page and then you can scroll through the pages. That's what we have implemented and what we usually deliver. If we need some mass export, I think we need some different implementation. We cannot reuse what we currently have. We need something more performant here. It seems good. What is Mark's size of page? Today it is 100 I think 100 items per page. And Tobias this is how does it work right now for service center? Yes. At least this is also what we have tested in the performance test. So we know it's working for sure we can make 1000 2000 but everything has to be tested if it's working. Not sure if 50,000 would work and how large the payload is. So are we talking about 10 MB, 5 MB or 100 MB of payload? This has to be investigated there. So we need to hear requirements what should be achieved and then we can look what is possible. But again, I don't think it's a good option to try to fetch download for one country everything once so then we need for sure something different. Is it per customer per segmentation or what kind of price price loan? It is not for customer. It's for supplier as I think because prices for customer we calculate on our side. So they need they need the base data Tobias. Like for example, let's say what we do in our API every let's say a customer clicks and logins and he says okay, I would like to look for Germany diesel prices okay, today and tomorrow. So our API is pulling Germany diesel today tomorrow. These are the three things which we fetch what they are looking for is once or twice in a day, all the data which we have on Azure. So maybe like it's dumping of data technically. Correct me guys if I'm wrong. Like I think the data what we got at nine o'clock and give it to them in one go. And that's it. I think that's the best when actually clarified the situation. Yeah, I am here. I hear what's on the cells. And then you will have all the suppliers all the stations all the fuels, all the things. If it's already divided into customers, we can pick one customer or two or three according to like a region based on the region that this customer belongs to. If it's already similar prices for many different customers. We want to use as Santosh said, the basic price. So what we have currently implemented relies on segmentation and this assignment of a Euro price in Poland. That's the things we have implemented. Maybe Santosh you can support you. So when we're talking about patching extra times place data, we can fetch 25 segments. Okay, so for currently for Polish customers, we don't have segments. So they are all in one segment. They all are in one segment 11. So basically for them that 11 doesn't make any difference for them. What will matter is the base prices. I mean, all the prices in segments are the same. So we can only give them the price 11 and that's it for stations, countries, fuels. Which currency? Euro or? Local. Local. Because for Poland, we have Polish currencies. For Czech we have Czech currencies. For Germans, we have German currencies. So they will get the data. I have one more question. We're using what we have. So basically that's not possible then we need to a new implementation. I have one more question to Gregor and Tim. You guys need data only of Polish suppliers or of Poland? All Europe. Okay. Then I would say in that context, it's the segment 11 prices for whole Europe. Or Tobias, do you think we can ask? I don't know. Because you guys are also building these pipelines, we can give you the data dump instead of building the API. Okay. Instead of building the API. What do you think? Like we build a data dump from Ashish to Gregor. Okay, or maybe my terminology is not correct. But whatever data we get from in Azure Cloud, that's the whole data we have for every single customer, every single segment, everything, everything, everything. And then in the login of the customer, it decides from our service center, the login decides what he can see. So if the customer is in segment 11, he has a segment 11 associated. So that segment customer is associated with the segment. And then it talks in the same segmentation prices as we have in the data. And that's what we have API. So API pulls the data what the customer belongs to, and the segments he belongs to. That's how they do. But you guys don't need the segmentation data, you guys need everything. Or do we need to build means I'm not able to do can we build a separate API like a different API, which is which has nothing to do with our service center API, completely a new thing that okay, whenever there is a data refresh in cloud, we give the data but that will be a then you guys need to define the requirements, give it to Tobias what all you need, and then only he can he will be able to produce it. I mean, what I understood is that we need that mass import. And this is something what is usually not done via a REST API. So not sure if the database pipeline, what I can do is maybe the better solution here. I would look into that direction. Then I have one more question to Gregor and the IT colleagues sitting here. Do you guys have access to Azure Cloud? Yes, of course. Good. Then I think Tobias the easiest way is to build something between these clouds and put the data in the in the Azure Cloud for for Polish colleagues. That is the easiest then. So for example, whenever Ashish gets the data, it's an Azure Cloud. We build a pipeline for you guys, we have some folders. And based on the requirements, he can build a query and out of query, whenever there is a refresh, it refreshes your folder. And from there on you guys are on your page, then you pull the data from the folder that you have. Because I don't think so we will be able to pull up millions of lines through APIs. Because the data which we have... There is no... Sorry, sorry, sorry. I think we don't understand each other because we need only prices on sites. Or maybe I'm wrong. But we think about prices on sites, on petrol stations. We're not talking about prices for each customer. We don't have one price per station now anymore. We are in a different world now. We have 26 prices on each station, multiplied by number of 26, number of the customers. So that's how many lines we generate actually. So it's not one price, one fuel, that that is old thing now. So whenever we pull the data now, we have millions of lines. Now, it's multiplied by 26. Yeah, but you've said that our customers are in the same group and and probably for our customers, it's only one price. Yes. But I mean, in the database, the database has everything. So we need to build in something which pulls only the prices for Poland. And we also need to think futuristically, because this new thing will also come to Poland in a couple of years. So maybe in one or two years. At that point of time, we need to think in such a way that whenever we change the things, if there is a dynamic pricing implementation in Poland, we should be able to get the data, the same data, but on a customer level now. So not one price, but all the 26 price together with the same methodology we are using for Germany. So not only currently, it's very easy, it will be more or less my understanding is, we can connect the Azure cloud to your cloud and you guys will get all the prices which you are getting. It's it's as in a simple terms. Okay, but finally, we will take only prices from the one group if in the future, we'll split on the two or three groups will need to take from this two or three groups. I think that's situation is that we'll have all 26 groups for our customers. It's rather unrealistic. Just got a question. If in the same group, Estonia or Lithuania customers are or they already have something different? No, I think when we go live for Poland and Baltics, we will go together. Okay. So that's our idea. Because then we are in sync because Poland and Baltics are very much closely synced. So we'll use the same same time for going live in those countries. So my understanding is we can't build an API correct me if I'm wrong Tobias because it's not what we supposed to do for such mass import. So you need all the data and one set of file or payload. I will ask Ashish who is our Azure expert. And then we will need you from from it that we can give you this file in a in a some folder in Azure cloud because we have all the data in Azure. So we can do something with the query or something. It's a simple SQL query. But you have to be executed. It's not a file or just SQL query, fetch all the data, download it or I don't know. It's inserted into a different table. One question, is it really so difficult to filter only this one group because we are in such way we are taking only 24 times smaller file in such situation. If in the future we will increase the number of groups to two or three, it will be still three, not 24. And size of this file is radically smaller. And transfer is faster, etc, etc. And the question is only, is it possible in easy way to filter this whole database to take only what we really need? Because the rest will be garbage for us. 



Yeah, we will only get the segment 11 price. So you don't get 26 prices until we are in the new model. We will not have the old methodology. So you'll get only the data which you need. So when I create for segment 11 for Germany, we have around 30,000 records and 30,000 records with prices, prices. I guess you need a specific date maybe. Or is it always the current date or do you want to send me a date which way you want to fetch data? So anyway, 30,000 records as a rest payload. It's not what we should suppose to send via the traffic. So seven days will be okay like seven times from today. In my opinion, it's a question about first transfer and next year if we will get this prices on the daily base, it will be necessary only to get the current one. Well, in that case, we would need the latest date from the week period. It's not always it's gonna be like, maybe not. I don't know. I'm just basing on the last approach. Yeah, you're right. It can be a different way. Tobias, we need as actual prices you have. It means that the last price which were paid by our customer because we can imagine the stations which are not very popular and transactions that are happened once a month. For that reason, we need the last price, last available price. Of course, with the information which station it is and what is the date of this price. I thought it's just about just least prices. Prices or am I incorrect? Only in this process? Yes. There's gonna be only a price set up by UDA. I assume current day and tomorrow. The current day is the most important for us. Yes, so one day current date. Okay, so we need another meeting to catch up with our our edger guy. So I will schedule a call next week with him. And then we see how can we pull it from from there. Maybe some architect. Yeah, I could maybe tell you who is the right candidate here. Yeah. Because we are talking about mass data initialization. I would be the best approach. Yeah, I will check with one for what he suggests. And if you suggest okay, we can do it directly through Azure Cloud, then we will go to Azure Cloud. If you suggest something else, then we'll do it the way he suggests. Okay, great. Then thank you very much. And I wish you a nice day. Thank you. Thank you. Do you call? Yes. Thank you. Thank you guys. Bye. Bye.